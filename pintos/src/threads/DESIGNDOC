            +--------------------+
            |        CS 140      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+
                   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Benjamin Shapero <bshapero@stanford.edu>
James Whitbeck <jamesbw@stanford.edu>
Stewart MacGregor-Dennis <sadennis@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----
>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct alarm {
  struct list_elem elem;
  int64_t alarm_tick;    /* Wait until this tick to wake */
  struct semaphore sem;
};

--list_elem elem is for use with the list libary.
--sem is the semaphore downed by the thread when it goes to sleep.  The
  interrupt will wake the thread by upping this semaphore.
--alarm_tick is the tick when the interrupt will up the semaphore.

struct list alarm_list;

--alarm_list contains the alarm structs, one for each sleeping
thread.  This list is sorted by alarm_tick so the next alarm is at the head.

---- ALGORITHMS ----
>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When a thread calls timer_sleep(), the thread declares a new alarm,
initializes the semaphore and sets the alarm by assigning its wake up
tick.  Then the alarm is added to the alarm_list, which is checked by the
interrupt handler.  The handler at each tick will compare the first alarm
in the list with the current tick in the check_alarms function.  If the
alarm should be woken up, then it is, the alarm is removed from the list
and the next alarm is checked. If the first alarm is not ready to be woken
up, then the handler returns. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

In order for the handler to run as quickly as possible, the alarm list is
sorted so that the head of the list wakes up before any alarm behind it.
This means that if no alarm fires, the function returns immediately.  At
worst, this algorithm is linear in the number of alarms that are firing at
once.

Additionally, there is no need for alarms to be allocated to the heap
because the lifetime of the alarm is the lifetime of the call to
timer_sleep().  The handler does not need to spend time on memory management.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Because the only time when threads touch shared state is when new alarms
are added to the list, we disable interrupts while the call to
list_insert_ordered is running. Concurrent calls to timer_sleep will not
interfere while inside this critical section.   

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

We prevent timer interrupts by disabling interrupts while inserting alarms
into the alarm list.  The only interaction between timer_sleep and
timer_interrupt is in the alarm_list, so by isolating the insert, races
are avoided.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

This design was chosen because it has the simplest implementation that
functions correctly.  Because interrupts can't go to sleep, primitives
such as locks and conditions could not work.  Semaphores, however, allowed
for a thread to sleep and an interrupt to wake the thread up.

The most important implementation detail was deciding to order the
alarm_list so that checking alarms was as fast as possible.  The
additional cost of the ordered list is incurred during insertion.
calls to timer_sleep are rare compared to calls to timer_interrupt, so the
system can afford to charge the cost to the less frequent function.
timer_interrupt is called at every tick, so it had to be fast.  A previous
design had the alarms allocated to the heap and then later freed once the
thread had been woken up.  However, calls to free() take up to 2 ticks of
CPU time, meaning that ticks were lost whenever free is called.  Luckily,
allocating to the heap was unnecessary.  Although the code passed the
tests when timer_sleep did not disable interrupts, there was the potential
for race conditions, so adding the critical section was an obvious improvement.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct lock {
    struct thread *holder;      /* Thread holding lock (for debugging). */
    struct semaphore semaphore; /* Binary semaphore controlling access. */
    struct list_elem elem;
};

--elem is for a list of locks that each thread holds.

struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging
  purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */
    int priority;                       /* Priority. */
    int original_priority;              /* Original priority before
  donation */
    struct list_elem allelem;           /* List element for all threads
  list.*/
    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

    struct lock *lock_waited_on; /* If thread is blocked on a lock */
    struct list locks_held; /* All locks currently held */

    /* mlfqs. */
    int nice;
    int17_14t recent_cpu;

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */
};

--original_priority stores the thread's pre-donation priority. This allows
  the thread to revert to it's original priority during the release of a
  lock.
--lock_waited_on stores the lock that the current thread is waiting on.
--locks_held stores a list of locks that the current thread is
  holding. This allows the thread to make a correct priority change when
  releasing a lock.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The data structures involve a struct field to store the original priority of the
thread, and a list of locks being held by each thread. The list of locks held
lets the lock_release function check for if the current thread should
drop its priority to a waiting thread's level, rather than revert to its
original priority.

See nested_donation.png file for diagram of nested donation process.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

When calling sema_up, we loop through the list of waiter threads and 
unblock the one with the highest priority.
Lock_release leverages this too because it calls sema_up.
When signaling a condition variable, we loop through the waiting 
semaphore elems in a similar fashion and call sema_up on the one 
which holds the highest priority thread.
We make use of two comparator functions that are passed to 
list_max:
- thread_priority_comparator
- sema_elem_priority_comparator


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

If we try to acquire a lock that is already held by another thread 
(meaning the holder is not NULL), then we donate our priority to that 
thread by calling thread_donate_priority.
Within thread_donate_priority, we update the receiving thread's priority 
if it is higher than the previous value.
To handle nested donations, we check whether the thread we are 
donating to is also blocked waiting for a lock. In that case, 
we call thread_donate_priority recursively on that lock's holder thread.
In order for this mechanism to work, we added the lock_waited_on pointer 
in struct thread.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When lock_release is called, the lock->holder field is reset to NULL. The
lock is then removed from the list of locks held by the thread and
sema_down is called. Much of the checking for correct donation and
reverting back to a lower priority is done in a helper method called
thread_recalculate_donation; this method checks if there are any other
higher priority threads waiting on the lock that the current thread should
receive a donation from. This prevents the thread stepping back to its
original priority when there are higher priority threads waiting on its lock.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

When a thread calls thread_set_priority, it is trying to modify its 
own priority. However, some other thread may also be donation its 
priority to the first thread, by called thread_donate_priority.
A thread's priority has become a shared resource, so we make sure 
we have disabled interrupts whenever we access the priority field.

We could not use a lock to protect the priority field. Indeed, if 
thread A is trying to acquire a given lock L, and thread B already 
holds it, then thread A will try to donate its priority to thread B. 
If, to do so, it has to acquire a lock L_B, we may run into a deadlock.
Imagine thread B is about to call set_priority and will also need to 
acquire lock L_B. Then threads A and B are both trying to acquire 
locks L and L_B, but in reverse order. We have introduced circularity 
and we risk deadlock.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design for simplicity. For example, we could have had a new
priority field and started using that, yet by continuing to use the
priority field (and just storing the original priority) much of the code
could be left the same; the exchange only needed to be made during
donation and release. 

The other ways we considered included having a global list of locks and
cycling through them, as opposed to a list of locks for each
thread. However, this would create unnecessary global variables, and could
have a large amount of overhead needed to loop through all the locks
frequently. Therefore, the current design was opted for as it reduced the
need for global variables and it kept most of the computation local to the
thread and lock structs.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In fixed_point.h:

typedef int int17_14t;
Used to identify integers when they are in 17.14 fixed-point number 
representation.

In thread.h:

Added int nice to struct thread: each thread has a nice field for the
advanced scheduler.
Added int17_14t recent_cpu to struct thread: each thread keeps track of
its own recent_cpu in  17.14 fixed-point number representation.

In thread.c:

Add int17_14t load_avg: this is a global variable keeping track of the
average load for the advanced scheduler.

In timer.c:
#define MLFQS_PRI_UPDATE_FREQ 4: the number of ticks after which we have 
to update each thread's priority.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0   63  61  59      A
 4      4   0   0   62  61  59      A
 8      8   0   0   61  61  59      B
12      8   4   0   61  60  59      A
16      12  4   0   60  60  59      B
20      12  8   0   60  59  59      A
24      16  8   0   59  59  59      C
28      16  8   4   59  59  58      B
32      16  12  4   59  58  58      A
36      20  12  4   58  58  58      C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Yes. When the top priority is shared by several threads, for instance at 
tick 8 or tick 24, then the scheduler could pick any of them. I chose 
the thread that had been off the processor the longest. This matches 
the behavior of our scheduler, since list_max returns the first thread 
with the highest priority, and running threads are pushed to the back 
of the list of ready threads when they are done running.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

All the updating of recent_cpus, load average and priorities happens 
within the interrupt context. Indeed this is all driven by the timer 
interrupts and it is critical that they complete before the next timer 
interrupt. We are however careful only to call thread_update_priority 
on each thread every 4 ticks, and update the load average and recent_
cpus only every 100 ticks. If there are very many threads to update 
regularly, this may start affecting performance. Clearly, this is a 
tradeoff of between spending less time inside interrupt context and 
have more accurate scheduling measurements.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

We only used one queue for the ready threads instead of 64, one for 
each priority level. This has made our solution simpler, although the 
lookups for next_thread_to_run will be longer. This is a possible 
improvement.
Even with a single queue, perhaps we could have ensured that the queue 
was sorted so that lookups would be short.
The abstraction of the fixed-point arithmetic and typedef-ing a real 
number type made the algorithms more readable.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

We built a library of functions to perform all possible operations 
between ints and ints, real numbers and real numbers, ints and real 
numbers. We also created a new type, int17_14t, that is really just 
an integer, but helps understand when a fixed-point representation 
is being used.
This abstraction was useful especially for the more complicated 
cases like division and multiplication of real numbers. That way, 
the functions implementing the advanced scheduler have less complexity.
Also, once these functions were done, we could forget about how fixed-
point arithmetic works.
Having compiled functions instead of macros also made debugging in 
gdb easier.


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It was ok.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Yes. The priority scheduling.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

Not really.

>> Any other comments?

There is a small bug in the mlfqs-recent-1 test, making the recent_cpus 
off by 1 compared to the expected results, although it remains within 
the tolerance.
I brought this up with Stephen Rumble.